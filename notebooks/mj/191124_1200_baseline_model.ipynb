{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/batman/kaggle/kaggle-ashrae-DSR\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/batman/kaggle/kaggle-ashrae-DSR')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_UTILS_PACKAGE = Path(\"./src/utils\")\n",
    "assert PATH_UTILS_PACKAGE.exists(), \"Can't find {}\".format(PATH_UTILS_PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 13:47:38 : Logging started\n",
      "2019-11-24 13:47:38 : Kernel started 2019-11-24 13:47:38.974256\n"
     ]
    }
   ],
   "source": [
    "# Logging\n",
    "# =============================================================================\n",
    "import sys\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "\n",
    "# Set level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create formatter\n",
    "FORMAT = \"%(asctime)s : %(message)s\"\n",
    "DATE_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "\n",
    "# Create handler and assign\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "\n",
    "logging.info(\"Logging started\")\n",
    "\n",
    "logging.info(\"Kernel started {}\".format(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.ashrae_transformers as trfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our local imports\n",
    "import src.utils.ashrae_transformers as trfs\n",
    "from src.utils.utility_classes import Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BaseEstimator',\n",
       " 'FeatureSelector',\n",
       " 'TemporalTransformer',\n",
       " 'TransformerMixin',\n",
       " 'TypeSelector',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "# =============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import zipfile\n",
    "import gc\n",
    "import time\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import yaml\n",
    "import inspect\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "# =============================================================================\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# ML imports\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "print('numpy {} as np'.format(np.__version__))\n",
    "import pandas as pd\n",
    "print('pandas {} as pd'.format(pd.__version__))\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import sklearn as sk\n",
    "print('sklearn {} as sk'.format(sk.__version__))\n",
    "\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.decomposition\n",
    "import sklearn.compose\n",
    "import sklearn.utils\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "print(\"lightgbm\", lgb.__version__)\n",
    "import xgboost as xgb\n",
    "print(\"xgboost\", xgb.__version__)\n",
    "# from catboost import CatBoostClassifier\n",
    "import catboost as catb\n",
    "print(\"catboost\", catb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# =============================================================================\n",
    "import matplotlib as mpl\n",
    "print('matplotlib {} as mpl'.format(mpl.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib.pyplot as plt'.format())\n",
    "import seaborn as sns\n",
    "print('seaboarn {} as sns'.format(sns.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment\n",
    "SETTINGS = Map()\n",
    "SETTINGS.data = Map()\n",
    "SETTINGS.model = Map()\n",
    "\n",
    "SETTINGS.data.path_data_root = Path.cwd() / 'data' / 'feather'\n",
    "SETTINGS.data.use_ucf = True\n",
    "SETTINGS.data.path_output = Path.cwd() / 'output'\n",
    "\n",
    "SETTINGS.model.folds = 5\n",
    "SETTINGS.model.num_rounds=1000\n",
    "SETTINGS.control = Map()\n",
    "SETTINGS.control.debug = False\n",
    "\n",
    "logging.info(\"Settings:\".format())\n",
    "pprint(SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Train data"
   },
   "outputs": [],
   "source": [
    "logging.info(\" *** Step 2: Load data *** \".format())\n",
    "\n",
    "# Train\n",
    "train_df = pd.read_feather(SETTINGS.data.path_data_root / 'train.feather')\n",
    "logging.info(\"Loaded: train_df {} with {} buildings, {:0.1f} MB\".format(train_df.shape, train_df.loc[:, 'building_id'].nunique(), train_df.memory_usage().sum() / 1024 ** 2))\n",
    "r1 = train_df.head()\n",
    "\n",
    "# Test\n",
    "test_df = pd.read_feather(SETTINGS.data.path_data_root / 'test.feather')\n",
    "logging.info(\"Loaded: test_df {} with {} buildings, {:0.1f} MB\".format(test_df.shape, test_df.loc[:, 'building_id'].nunique(), test_df.memory_usage().sum() / 1024 ** 2))\n",
    "r2 = test_df.head()\n",
    "\n",
    "# Weather train\n",
    "weather_train_df = pd.read_feather(SETTINGS.data.path_data_root/'weather_train.feather')\n",
    "logging.info(\"Loaded: weather_train_df {}\".format(weather_train_df.shape))\n",
    "r = weather_train_df.head()\n",
    "\n",
    "# Weather test\n",
    "weather_test_df = pd.read_feather(SETTINGS.data.path_data_root/'weather_test.feather')\n",
    "logging.info(\"Loaded: weather_test_df {}\".format(weather_test_df.shape))\n",
    "\n",
    "# Meta\n",
    "building_meta_df = pd.read_feather(SETTINGS.data.path_data_root/'building_metadata.feather')\n",
    "logging.info(\"Loaded: building_meta_df {}\".format(building_meta_df.shape))\n",
    "r = building_meta_df.head()\n",
    "building_meta_df.set_index('building_id',inplace=True, drop=True)\n",
    "\n",
    "# Sample\n",
    "# sample_submission = pd.read_feather(os.path.join(SETTINGS.data.path_data_root, 'sample_submission.feather'))\n",
    "# logging.info(\"Loaded: sample_submission {}\".format(sample_submission.shape))\n",
    "# sample_submission = reduce_mem_usage(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# train_merge = train_df_data.merge(building_meta_df, on='building_id', how='left')\n",
    "# train_merge.memory_usage().sum() / 1024 ** 2\n",
    "# test_merge = test_df_data.merge(building_meta_df, on='building_id', how='left')\n",
    "# test_merge.memory_usage().sum() / 1024 ** 2\n",
    "#\n",
    "# train_df = train_merge.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')\n",
    "# train_df.memory_usage().sum() / 1024 ** 2\n",
    "# r = train_df.head()\n",
    "#\n",
    "# test_df = test_merge.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')\n",
    "# test_df.memory_usage().sum() / 1024 ** 2\n",
    "# r = test_df.head()\n",
    "\n",
    "# train2.info()\n",
    "\n",
    "# del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_date_time_cols(df):\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    logging.info(\"Added date column\".format())\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    logging.info(\"Added hour column\".format())\n",
    "    # df[\"day\"] = df[\"timestamp\"].dt.day\n",
    "    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n",
    "    logging.info(\"Added weekend column\".format())\n",
    "    df[\"month\"] = df[\"timestamp\"].dt.month\n",
    "    logging.info(\"Added month column\".format())\n",
    "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
    "    logging.info(\"Added dayofweek column\".format())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Add dates and times"
   },
   "outputs": [],
   "source": [
    "logging.info(\"Adding basic time features to train and test\".format())\n",
    "train_df = preprocess_date_time_cols(train_df)\n",
    "test_df = preprocess_date_time_cols(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Take the ln transform of the targets"
   },
   "outputs": [],
   "source": [
    "train_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])\n",
    "test_df['meter_reading_log1p'] = np.log1p(test_df['meter_reading'])\n",
    "logging.info(\"Added meter_reading_log1p [ln(1+x)] column\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # test_df['building_mean'] = test_df['building_id'].map(building_mean)\n",
    "    test_df['building_median'] = test_df['building_id'].map(building_median)\n",
    "    # test_df['building_min'] = test_df['building_id'].map(building_min)\n",
    "    # test_df['building_max'] = test_df['building_id'].map(building_max)\n",
    "    # test_df['building_std'] = test_df['building_id'].map(building_std)\n",
    "\n",
    "    print('preprocessing weather...')\n",
    "    weather_test_df = weather_test_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\n",
    "    weather_test_df.groupby('site_id').apply(lambda group: group.isna().sum())\n",
    "\n",
    "    # add_lag_feature(weather_test_df, window=3)\n",
    "    # add_lag_feature(weather_test_df, window=72)\n",
    "\n",
    "    print('reduce mem usage...')\n",
    "    reduce_mem_usage(test_df, use_float16=True)\n",
    "    reduce_mem_usage(weather_test_df, use_float16=True)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "    logging.info(\"Removing building site=0\".format())\n",
    "    building_meta_df[building_meta_df.site_id == 0]\n",
    "    train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n",
    "\n",
    "\n",
    "logging.info(\" *** Step 3: Feature engineering *** \".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Now, Let's try building GBDT (Gradient Boost Decision Tree) model to predict `meter_reading_log1p`. I will try using LightGBM in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add time feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features introduced in https://www.kaggle.com/ryches/simple-lgbm-solution by @ryches\n",
    "\n",
    "Features that are likely predictive:\n",
    "\n",
    "#### Weather\n",
    "\n",
    "- time of day\n",
    "- holiday\n",
    "- weekend\n",
    "- cloud_coverage + lags\n",
    "- dew_temperature + lags\n",
    "- precip_depth + lags\n",
    "- sea_level_pressure + lags\n",
    "- wind_direction + lags\n",
    "- wind_speed + lags\n",
    "\n",
    "#### Train\n",
    "\n",
    "- max, mean, min, std of the specific building historically\n",
    "\n",
    "\n",
    "\n",
    "However we should be careful of putting time feature, since we have only 1 year data in training,\n",
    "including `date` makes overfiting to training data.\n",
    "\n",
    "How about `month`? It may be better to check performance by cross validation.\n",
    "I go not using this data in this kernel for robust modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(SETTINGS)\n",
    "# sort train. i dont know it is best\n",
    "# if use_ucf:\n",
    "#     train_df = train_df.sort_values('month')\n",
    "#     train_df = train_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Adding building_median log1p feature\".format())\n",
    "\n",
    "df_group = train_df.groupby('building_id')['meter_reading_log1p']\n",
    "#building_mean = df_group.mean().astype(np.float16)\n",
    "building_median = df_group.median().astype(np.float16)\n",
    "#building_min = df_group.min().astype(np.float16)\n",
    "#building_max = df_group.max().astype(np.float16)\n",
    "#building_std = df_group.std().astype(np.float16)\n",
    "\n",
    "#train_df['building_mean'] = train_df['building_id'].map(building_mean)\n",
    "train_df['building_median'] = train_df['building_id'].map(building_median)\n",
    "#train_df['building_min'] = train_df['building_id'].map(building_min)\n",
    "#train_df['building_max'] = train_df['building_id'].map(building_max)\n",
    "#train_df['building_std'] = train_df['building_id'].map(building_std)\n",
    "del df_group\n",
    "logging.info(\"Added building_median log1p feature\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Nan value in weather dataframe by interpolation\n",
    "\n",
    "\n",
    "weather data has a lot of NaNs!!\n",
    "\n",
    "![](http://)I tried to fill these values by **interpolating** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Interpolating over NaNs in weather_train_df\".format())\n",
    "# weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())\n",
    "weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems number of nan has reduced by `interpolate` but some property has never appear in specific `site_id`, and nan remains for these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lags\n",
    "\n",
    "Adding some lag feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_feature(weather_df, window=3):\n",
    "    group_df = weather_df.groupby('site_id')\n",
    "    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "    rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "    lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "    lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "    lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "    lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "    for col in cols:\n",
    "        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "        weather_df[f'{col}_std_lag{window}'] = lag_std[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip lag feature to save memory\n",
    "#add_lag_feature(weather_train_df, window=3)\n",
    "#add_lag_feature(weather_train_df, window=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize primary_use column to reduce memory on merge...\n",
    "logging.info(\"Convert primary_use to Categorical type\".format())\n",
    "primary_use_list = building_meta_df['primary_use'].unique()\n",
    "primary_use_dict = {key: value for value, key in enumerate(primary_use_list)}\n",
    "print('primary_use_dict: ', primary_use_dict)\n",
    "building_meta_df['primary_use'] = building_meta_df['primary_use'].map(primary_use_dict)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_mem_usage(train_df, use_float16=True)\n",
    "reduce_mem_usage(building_meta_df, use_float16=True)\n",
    "reduce_mem_usage(weather_train_df, use_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "building_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "category_cols = ['building_id', 'site_id', 'primary_use']  # , 'meter'\n",
    "feature_cols = ['square_feet', 'year_built'] + [\n",
    "    'hour', 'weekend',  # 'month' , 'dayofweek'\n",
    "    'building_median'] + [\n",
    "                   'air_temperature', 'cloud_coverage',\n",
    "                   'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                   'wind_direction', 'wind_speed', ]\n",
    "\n",
    "\n",
    "#     'air_temperature_mean_lag72',\n",
    "#     'air_temperature_max_lag72', 'air_temperature_min_lag72',\n",
    "#     'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n",
    "#     'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n",
    "#     'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72',\n",
    "#     'wind_speed_mean_lag72', 'air_temperature_mean_lag3',\n",
    "#     'air_temperature_max_lag3',\n",
    "#     'air_temperature_min_lag3', 'cloud_coverage_mean_lag3',\n",
    "#     'dew_temperature_mean_lag3',\n",
    "#     'precip_depth_1_hr_mean_lag3', 'sea_level_pressure_mean_lag3',\n",
    "#     'wind_direction_mean_lag3', 'wind_speed_mean_lag3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "To win in kaggle competition, how to evaluate your model is important.\n",
    "What kind of cross validation strategy is suitable for this competition? This is time series data, so it is better to consider time-splitting.\n",
    "\n",
    "However this notebook is for simple tutorial, so I will proceed with KFold splitting without shuffling, so that at least near-term data is not included in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_X_y(train_df, target_meter):\n",
    "    target_train_df = train_df[train_df['meter'] == target_meter]\n",
    "    target_train_df = target_train_df.merge(building_meta_df, on='building_id', how='left')\n",
    "    target_train_df = target_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')\n",
    "    X_train = target_train_df[feature_cols + category_cols]\n",
    "    y_train = target_train_df['meter_reading_log1p'].values\n",
    "\n",
    "    del target_train_df\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds = 5\n",
    "seed = 666\n",
    "shuffle = False\n",
    "kf = sk.model_selection.KFold(n_splits=SETTINGS.model.folds, shuffle=shuffle, random_state=seed)\n",
    "oof_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model by each meter type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_meter = 0\n",
    "X_train, y_train = create_X_y(train_df, target_meter=target_meter)\n",
    "y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "gc.collect()\n",
    "print('target_meter', target_meter, X_train.shape)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "print('cat_features', cat_features)\n",
    "\n",
    "logging.info(\" *** Step 4: Build model *** \".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fit_lgbm(train, val, devices=(-1,), seed=None, cat_features=None, num_rounds=1500, lr=0.1, bf=0.1):\n",
    "    \"\"\"Train Light GBM model\"\"\"\n",
    "    X_train, y_train = train\n",
    "    X_valid, y_valid = val\n",
    "    metric = 'l2'\n",
    "    params = {'num_leaves': 31,\n",
    "              'objective': 'regression',\n",
    "              #               'max_depth': -1,\n",
    "              'learning_rate': lr,\n",
    "              \"boosting\": \"gbdt\",\n",
    "              \"bagging_freq\": 5,\n",
    "              \"bagging_fraction\": bf,\n",
    "              \"feature_fraction\": 0.9,\n",
    "              \"metric\": metric,\n",
    "              #               \"verbosity\": -1,\n",
    "              #               'reg_alpha': 0.1,\n",
    "              #               'reg_lambda': 0.3\n",
    "              }\n",
    "    device = devices[0]\n",
    "    if device == -1:\n",
    "        # use cpu\n",
    "        pass\n",
    "    else:\n",
    "        # use gpu\n",
    "        print(f'using gpu device_id {device}...')\n",
    "        params.update({'device': 'gpu', 'gpu_device_id': device})\n",
    "\n",
    "    params['seed'] = seed\n",
    "\n",
    "    early_stop = 20\n",
    "    verbose_eval = 20\n",
    "\n",
    "    d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n",
    "    d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_features)\n",
    "    watchlist = [d_train, d_valid]\n",
    "\n",
    "    print('training LGB:')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=SETTINGS.model.num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "\n",
    "    # predictions\n",
    "    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "    print('best_score', model.best_score)\n",
    "    log = {'train/mae': model.best_score['training']['l2'],\n",
    "           'valid/mae': model.best_score['valid_1']['l2']}\n",
    "    return model, y_pred_valid, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "models0 = []\n",
    "for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    train_data = X_train.iloc[train_idx, :], y_train[train_idx]\n",
    "    valid_data = X_train.iloc[valid_idx, :], y_train[valid_idx]\n",
    "\n",
    "    print('train', len(train_idx), 'valid', len(valid_idx))\n",
    "    #     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n",
    "    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n",
    "                                        num_rounds=SETTINGS.model.num_rounds, lr=0.05, bf=0.7)\n",
    "    y_valid_pred_total[valid_idx] = y_pred_valid\n",
    "    models0.append(model)\n",
    "    gc.collect()\n",
    "    if SETTINGS.control.debug:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_train)\n",
    "sns.distplot(y_valid_pred_total)\n",
    "\n",
    "oof0 = sk.metrics.mean_squared_error(y_train, y_valid_pred_total)\n",
    "oof_total += oof0 * len(y_train)\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model):\n",
    "    importance_df = pd.DataFrame(model.feature_importance(),\n",
    "                                 index=feature_cols + category_cols,\n",
    "                                 columns=['importance']).sort_values('importance')\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    importance_df.plot.barh(ax=ax)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_meter = 1\n",
    "X_train, y_train = create_X_y(train_df, target_meter=target_meter)\n",
    "y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "gc.collect()\n",
    "print('target_meter', target_meter, X_train.shape)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "print('cat_features', cat_features)\n",
    "\n",
    "models1 = []\n",
    "for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    train_data = X_train.iloc[train_idx ,:], y_train[train_idx]\n",
    "    valid_data = X_train.iloc[valid_idx ,:], y_train[valid_idx]\n",
    "\n",
    "    print('train', len(train_idx), 'valid', len(valid_idx))\n",
    "    #     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n",
    "    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=SETTINGS.model.num_rounds,\n",
    "                                        lr=0.05, bf=0.5)\n",
    "    y_valid_pred_total[valid_idx] = y_pred_valid\n",
    "    models1.append(model)\n",
    "    gc.collect()\n",
    "    if SETTINGS.control.debug:\n",
    "        break\n",
    "\n",
    "sns.distplot(y_train)\n",
    "sns.distplot(y_valid_pred_total)\n",
    "\n",
    "oof1 = sk.metrics.mean_squared_error(y_train, y_valid_pred_total)\n",
    "oof_total += oof1 * len(y_train)\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_meter = 2\n",
    "X_train, y_train = create_X_y(train_df, target_meter=target_meter)\n",
    "y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "\n",
    "gc.collect()\n",
    "print('target_meter', target_meter, X_train.shape)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "print('cat_features', cat_features)\n",
    "\n",
    "models2 = []\n",
    "for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    train_data = X_train.iloc[train_idx ,:], y_train[train_idx]\n",
    "    valid_data = X_train.iloc[valid_idx ,:], y_train[valid_idx]\n",
    "\n",
    "    print('train', len(train_idx), 'valid', len(valid_idx))\n",
    "    #     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n",
    "    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n",
    "                                        num_rounds=SETTINGS.model.num_rounds, lr=0.05, bf=0.8)\n",
    "    y_valid_pred_total[valid_idx] = y_pred_valid\n",
    "    models2.append(model)\n",
    "    gc.collect()\n",
    "    if SETTINGS.control.debug:\n",
    "        break\n",
    "\n",
    "sns.distplot(y_train)\n",
    "sns.distplot(y_valid_pred_total)\n",
    "\n",
    "oof2 = sk.metrics.mean_squared_error(y_train, y_valid_pred_total)\n",
    "oof_total += oof2 * len(y_train)\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_meter = 3\n",
    "X_train, y_train = create_X_y(train_df, target_meter=target_meter)\n",
    "y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "\n",
    "gc.collect()\n",
    "print('target_meter', target_meter, X_train.shape)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "print('cat_features', cat_features)\n",
    "\n",
    "models3 = []\n",
    "for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    train_data = X_train.iloc[train_idx ,:], y_train[train_idx]\n",
    "    valid_data = X_train.iloc[valid_idx ,:], y_train[valid_idx]\n",
    "\n",
    "    print('train', len(train_idx), 'valid', len(valid_idx))\n",
    "    #     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n",
    "    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=SETTINGS.model.num_rounds,\n",
    "                                        lr=0.03, bf=0.9)\n",
    "    y_valid_pred_total[valid_idx] = y_pred_valid\n",
    "    models3.append(model)\n",
    "    gc.collect()\n",
    "    if SETTINGS.control.debug:\n",
    "        break\n",
    "\n",
    "sns.distplot(y_train)\n",
    "sns.distplot(y_valid_pred_total)\n",
    "\n",
    "oof3 = sk.metrics.mean_squared_error(y_train, y_valid_pred_total)\n",
    "oof_total += oof3 * len(y_train)\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# OOF SCOREs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('oof score meter0 =', np.sqrt(oof0))\n",
    "print ('oof score meter1 =', np.sqrt(oof1))\n",
    "print ('oof score meter2 =', np.sqrt(oof2))\n",
    "print ('oof score meter3 =', np.sqrt(oof3))\n",
    "print ('oof score total  =', np.sqrt(oof_total / len(train_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "del train_df, weather_train_df, building_meta_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def create_X(test_df, target_meter):\n",
    "    target_test_df = test_df[test_df['meter'] == target_meter]\n",
    "    target_test_df = target_test_df.merge(building_meta_df, on='building_id', how='left')\n",
    "    target_test_df = target_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')\n",
    "    X_test = target_test_df[feature_cols + category_cols]\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def pred(X_test, models, batch_size=1000000):\n",
    "    iterations = (X_test.shape[0] + batch_size -1) // batch_size\n",
    "    print('iterations', iterations)\n",
    "\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'predicting {i}-th model')\n",
    "        for k in tqdm.tqdm(range(iterations)):\n",
    "            y_pred_test = model.predict(X_test[k * batch_size:(k + 1) * batch_size], num_iteration=model.best_iteration)\n",
    "            y_test_pred_total[k * batch_size:(k + 1) * batch_size] += y_pred_test\n",
    "\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = create_X(test_df, target_meter=0)\n",
    "gc.collect()\n",
    "\n",
    "y_test0 = pred(X_test, models0)\n",
    "\n",
    "sns.distplot(y_test0)\n",
    "\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = create_X(test_df, target_meter=1)\n",
    "gc.collect()\n",
    "\n",
    "y_test1 = pred(X_test, models1)\n",
    "sns.distplot(y_test1)\n",
    "\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = create_X(test_df, target_meter=2)\n",
    "gc.collect()\n",
    "\n",
    "y_test2 = pred(X_test, models2)\n",
    "sns.distplot(y_test2)\n",
    "\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = create_X(test_df, target_meter=3)\n",
    "gc.collect()\n",
    "\n",
    "y_test3 = pred(X_test, models3)\n",
    "sns.distplot(y_test3)\n",
    "\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.loc[test_df['meter'] == 0, 'meter_reading'] = np.expm1(y_test0)\n",
    "sample_submission.loc[test_df['meter'] == 1, 'meter_reading'] = np.expm1(y_test1)\n",
    "sample_submission.loc[test_df['meter'] == 2, 'meter_reading'] = np.expm1(y_test2)\n",
    "sample_submission.loc[test_df['meter'] == 3, 'meter_reading'] = np.expm1(y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(SETTINGS.data.path_output / 'submission.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace to UCF data\n",
    "if 0:\n",
    "    # %%\n",
    "    leak_score = 0\n",
    "\n",
    "    leak_df = pd.read_pickle(ucf_root / 'site0.pkl')\n",
    "    leak_df['meter_reading'] = leak_df.meter_reading_scraped\n",
    "    leak_df.drop(['meter_reading_original', 'meter_reading_scraped'], axis=1, inplace=True)\n",
    "    leak_df.fillna(0, inplace=True)\n",
    "    leak_df = leak_df[leak_df.timestamp.dt.year > 2016]\n",
    "    leak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0  # remove large negative values\n",
    "\n",
    "    sample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0\n",
    "\n",
    "    for bid in leak_df.building_id.unique():\n",
    "        temp_df = leak_df[(leak_df.building_id == bid)]\n",
    "        for m in temp_df.meter.unique():\n",
    "            v0 = sample_submission.loc[(test_df.building_id == bid) & (test_df.meter == m), 'meter_reading'].values\n",
    "            v1 = temp_df[temp_df.meter == m].meter_reading.values\n",
    "\n",
    "            leak_score += sk.metrics.mean_squared_error(np.log1p(v0), np.log1p(v1)) * len(v0)\n",
    "\n",
    "            sample_submission.loc[(test_df.building_id == bid) & (test_df.meter == m), 'meter_reading'] = temp_df[\n",
    "                temp_df.meter == m].meter_reading.values\n",
    "\n",
    "    # %%\n",
    "    if not SETTINGS.control.debug:\n",
    "        sample_submission.to_csv('submission_ucf_replaced.csv', index=False, float_format='%.4f')\n",
    "\n",
    "    # %%\n",
    "    sample_submission.head()\n",
    "\n",
    "    # %%\n",
    "    np.log1p(sample_submission['meter_reading']).hist()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # UCF score\n",
    "\n",
    "    # %%\n",
    "    print('UCF score = ', np.sqrt(leak_score / len(leak_df)))\n",
    "\n",
    "    # %%\n",
    "    plot_feature_importance(models0[1])\n",
    "\n",
    "    # %%\n",
    "    plot_feature_importance(models1[1])\n",
    "\n",
    "    # %%\n",
    "    plot_feature_importance(models2[1])\n",
    "\n",
    "    # %%\n",
    "    plot_feature_importance(models3[1])\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # References\n",
    "    #\n",
    "    # These kernels inspired me to write this kernel, thank you for sharing!\n",
    "    #\n",
    "    #  - https://www.kaggle.com/rishabhiitbhu/ashrae-simple-eda\n",
    "    #  - https://www.kaggle.com/isaienkov/simple-lightgbm\n",
    "    #  - https://www.kaggle.com/ryches/simple-lgbm-solution"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,_kg_hide-input,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "kaggle_ashrae_dsr",
   "language": "python",
   "name": "kaggle_ashrae_dsr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
